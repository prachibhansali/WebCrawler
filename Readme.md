A web crawler which crawls over 20,000 documents over the internet with the following functionalities:
1. Maintaing Politeness Policy while crawling every link
2. Maintaining priority queue for choosing URL to parse based on the maximum in-links count
3. URL Normalization
4. Document extraction and processing
5. Building an index on ElasticSearch with the retrieved documents

Built using Go Lang
